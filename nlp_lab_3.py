# -*- coding: utf-8 -*-
"""Nlp-lab-3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gV-ntz43m3w3ZZ08GxaIBaSWYTmGJBUI
"""

import nltk
from nltk.corpus import brown
from nltk.tokenize import word_tokenize

nltk.download('brown')
nltk.download('punkt')
nltk.download('punkt_tab')

category = 'news'
words_in_category = brown.words(categories=category)

text_str = ' '.join(words_in_category)

tokens = word_tokenize(text_str)

print(f"Category Selected: {category}")
print("First 20 Tokens:", tokens[:20])
print(f"Total Tokens: {len(tokens)}")

import nltk
from nltk.corpus import brown
from nltk.tokenize import word_tokenize
from nltk.probability import FreqDist
import matplotlib.pyplot as plt
from wordcloud import WordCloud

nltk.download('brown')
nltk.download('punkt')

words = brown.words(categories='news')

text = ' '.join(words)
tokens = word_tokenize(text)

fdist = FreqDist(tokens)

top_words = fdist.most_common(20)

plt.bar([w for w, _ in top_words], [c for _, c in top_words])
plt.xticks(rotation=45)
plt.title("Top 20 Words (Brown Corpus - News)")
plt.show()

wordcloud = WordCloud(width=800, height=400, background_color='white')\
    .generate_from_frequencies(dict(top_words))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()

import nltk
from nltk.corpus import brown
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
from nltk.probability import FreqDist

nltk.download('brown')
nltk.download('punkt')
nltk.download('punkt_tab') # Download missing resource

words = brown.words(categories='news')
text = ' '.join(words)
tokens = word_tokenize(text)

ps = PorterStemmer()
stemmed_tokens = [ps.stem(w.lower()) for w in tokens if w.isalpha()]

fdist = FreqDist(stemmed_tokens)

for word, freq in fdist.most_common(20):
    print(f"{word}: {freq}")

import nltk
from nltk.corpus import brown
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from nltk.probability import FreqDist

nltk.download('brown')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('omw-1.4')

words = brown.words(categories='news')
text = ' '.join(words)
tokens = word_tokenize(text)

lemmatizer = WordNetLemmatizer()
lemmatized_tokens = [lemmatizer.lemmatize(w.lower()) for w in tokens if w.isalpha()]

fdist = FreqDist(lemmatized_tokens)

for word, freq in fdist.most_common(20):
    print(f"{word}: {freq}")

import nltk
from nltk.corpus import brown
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer, WordNetLemmatizer
from nltk.probability import FreqDist
import matplotlib.pyplot as plt
import numpy as np

nltk.download('brown')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('omw-1.4')

words = brown.words(categories='news')
text = ' '.join(words)
tokens = [w.lower() for w in word_tokenize(text) if w.isalpha()]

ps = PorterStemmer()
lemmatizer = WordNetLemmatizer()

stemmed_tokens = [ps.stem(w) for w in tokens]
lemmatized_tokens = [lemmatizer.lemmatize(w) for w in tokens]

stem_freq = FreqDist(stemmed_tokens)
lemma_freq = FreqDist(lemmatized_tokens)

common_words = list({w for w, _ in stem_freq.most_common(30)} | {w for w, _ in lemma_freq.most_common(30)})
common_words = common_words[:15]

stem_counts = [stem_freq[w] for w in common_words]
lemma_counts = [lemma_freq[w] for w in common_words]

x = np.arange(len(common_words))
width = 0.35

plt.figure(figsize=(10, 5))
plt.bar(x - width/2, stem_counts, width, label='Stemming')
plt.bar(x + width/2, lemma_counts, width, label='Lemmatization')

plt.xticks(x, common_words, rotation=45)
plt.ylabel('Frequency')
plt.title('Stemming vs Lemmatization (Brown Corpus - News)')
plt.legend()
plt.tight_layout()
plt.show()